{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83951c0-4675-402b-b085-1a6234776f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CelebA\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "500a119c-671f-43c4-98fd-b3972c0ce05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_CelebA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_CelebA, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size= 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size= 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size= 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size= 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size= 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.z_mean = nn.Linear(512*4, 128)\n",
    "        self.z_log_var = nn.Linear(512*4, 128)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(128, 512*4),\n",
    "            nn.Unflatten(1, (512, 2, 2)),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),            \n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 3, kernel_size= 3, padding= 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        mu = self.z_mean(enc)\n",
    "        lv = self.z_log_var(enc)\n",
    "        lat = self.reparam(mu, lv)\n",
    "        dec = self.decoder(lat)\n",
    "        \n",
    "        return dec, mu, lv\n",
    "\n",
    "    def reparam(self, mu, lv):\n",
    "        std = torch.exp(0.5 * lv)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + std * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c3c3b1-7c72-4f5c-8360-5b5d3094b8a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m      2\u001b[0m     [\n\u001b[1;32m      3\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     ]\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[43mCelebA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/media/airesearch/khoibaocon/git/data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m train_dl \u001b[38;5;241m=\u001b[39m DataLoader(trainset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n\u001b[1;32m     12\u001b[0m validset \u001b[38;5;241m=\u001b[39m CelebA(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/media/airesearch/khoibaocon/git/data\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[0;32m/media/airesearch/khoibaocon/git/DecodeItAll/.env/lib/python3.9/site-packages/torchvision/datasets/celeba.py:80\u001b[0m, in \u001b[0;36mCelebA.__init__\u001b[0;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_transform is specified but target_type is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/media/airesearch/khoibaocon/git/DecodeItAll/.env/lib/python3.9/site-packages/torchvision/datasets/celeba.py:150\u001b[0m, in \u001b[0;36mCelebA.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (file_id, md5, filename) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list:\n\u001b[0;32m--> 150\u001b[0m     \u001b[43mdownload_file_from_google_drive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m extract_archive(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_align_celeba.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/media/airesearch/khoibaocon/git/DecodeItAll/.env/lib/python3.9/site-packages/torchvision/datasets/utils.py:246\u001b[0m, in \u001b[0;36mdownload_file_from_google_drive\u001b[0;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[1;32m    243\u001b[0m         api_response, content \u001b[38;5;241m=\u001b[39m _extract_gdrive_api_response(response)\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m api_response \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuota exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe daily quota of the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is exceeded and it \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be downloaded. This is a limitation of Google Drive \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand can only be overcome by trying again later.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m     _save_response_content(content, fpath)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# In case we deal with an unhandled GDrive API response, the file should be smaller than 10kB and contain only text\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The daily quota of the file img_align_celeba.zip is exceeded and it can't be downloaded. This is a limitation of Google Drive and can only be overcome by trying again later."
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(148),\n",
    "        transforms.Resize(64),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = CelebA(root='/media/airesearch/khoibaocon/git/data', split='train', download=True, transform=transform)\n",
    "train_dl = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=24)\n",
    "validset = CelebA(root='/media/airesearch/khoibaocon/git/data', split='valid', download=True, transform=transform)\n",
    "valid_dl = DataLoader(validset, batch_size=64, shuffle=False, num_workers=24)\n",
    "testset = CelebA(root='/media/airesearch/khoibaocon/git/data', split='test', download=True, transform=transform)\n",
    "test_dl = DataLoader(testset, batch_size=64, shuffle=False, num_workers=24)\n",
    "\n",
    "print(len(trainset), len(validset), len(testset))\n",
    "print(len(train_dl), len(valid_dl), len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6cd11c-330d-4fd6-9af5-4afa8813ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kls(mu, logvar):\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    return torch.mean(kld_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b305b95-bf53-4184-8250-fc6185528c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\", index = 0)\n",
    "\n",
    "model = CNN_CelebA().to(device)\n",
    "\n",
    "optimizer = Adam(params = model.parameters(), lr = 0.005)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "kld_w = 64/len(trainset)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, epochs*len(train_dl))\n",
    "\n",
    "recon_loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "def loss_fn(recon_x, x):\n",
    "    return torch.mean(torch.sum(recon_loss(recon_x, x), dim=(1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48f956-f65d-4a83-8d32-060ffe4d33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    tr_total_loss = {\n",
    "        \"rec_loss\" : 0,\n",
    "        \"kl_loss\" : 0\n",
    "    }\n",
    "    for train_img, _ in tqdm(train_dl):\n",
    "        train_img = train_img.to(device)\n",
    "\n",
    "        gen_img, train_mu, train_lv = model(train_img)\n",
    "        train_rec_loss = loss_fn(gen_img, train_img)\n",
    "        train_kl_loss = gaussian_kls(train_mu, train_lv)\n",
    "        train_loss = train_rec_loss + kld_w * train_kl_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_total_loss[\"rec_loss\"] += train_rec_loss.item()\n",
    "        tr_total_loss[\"kl_loss\"] += train_kl_loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        va_total_loss = {\n",
    "            \"rec_loss\" : 0,\n",
    "            \"kl_loss\" : 0\n",
    "        }\n",
    "        for valid_img, _ in tqdm(test_dl):\n",
    "            valid_img = valid_img.to(device)\n",
    "\n",
    "            gen_img, valid_mu, valid_lv = model(valid_img)\n",
    "            valid_rec_loss = loss_fn(gen_img, valid_img)\n",
    "            valid_kl_loss = gaussian_kls(valid_mu, valid_lv)\n",
    "            valid_loss = valid_rec_loss + valid_kl_loss\n",
    "\n",
    "            va_total_loss[\"rec_loss\"] += valid_rec_loss.item()\n",
    "            va_total_loss[\"kl_loss\"] += valid_kl_loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch} - TrainRecLoss: {tr_total_loss['rec_loss']/len(train_dl)} - ValidRecLoss: {va_total_loss['rec_loss']/len(test_dl)}\")\n",
    "    print(f\"Epoch: {epoch} - TrainDivLoss: {tr_total_loss['kl_loss']/len(train_dl)} - ValidDivLoss: {va_total_loss['kl_loss']/len(test_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f337f4-4f19-40d2-a663-e43132f051c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c0d61-6101-493a-85eb-05c272718da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input, _ = next(iter(test_dl))\n",
    "test_input = test_input.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recons, _, _ = model(test_input)\n",
    "\n",
    "grid = make_grid(recons)\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19359e6-e431-4723-9fb4-583197bcf292",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(144, 128).to(device)\n",
    "\n",
    "samples = model.decoder(z)\n",
    "\n",
    "grid = make_grid(samples, nrow=12)\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa570ad-77b6-4801-b614-49b0fd7533a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
