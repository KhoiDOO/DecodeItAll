{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b3294b8-960f-48d5-98bd-c7a23c0694b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "141db9c7-23bb-4baa-a422-6606eeeb4c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "25 5\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = CIFAR10(root='~/data',  train=True, download=True, transform=transform)\n",
    "train_dl = DataLoader(trainset, batch_size=2048, shuffle=True, num_workers=2)\n",
    "testset = CIFAR10(root='~/data', train=False, download=True, transform=transform)\n",
    "test_dl = DataLoader(testset, batch_size=2048, shuffle=False, num_workers=2)\n",
    "\n",
    "print(len(train_dl), len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32558f01-3e0e-42a1-bd9f-a070fbb092ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(channel_num, channel_num, 5, stride = 1, padding=2),\n",
    "\t\t\tnn.BatchNorm2d(channel_num),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(channel_num, channel_num, 5, stride = 1, padding=2),\n",
    "\t\t\tnn.BatchNorm2d(channel_num),\n",
    "\t\t)\n",
    "        self.relu = nn.ReLU()\n",
    "        torch.nn.init.kaiming_normal_(self.conv_block1[0].weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_block2[0].weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x + residual\n",
    "        out = self.relu(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751a5b87-e24f-438b-935f-8881be2a3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTransBlock(nn.Module):\n",
    "    def __init__(self, channel_num):\n",
    "        super(BasicTransBlock, self).__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(channel_num, channel_num, 5, stride = 1, padding=2),\n",
    "\t\t\tnn.BatchNorm2d(channel_num),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t)\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(channel_num, channel_num, 5, stride = 1, padding=2),\n",
    "\t\t\tnn.BatchNorm2d(channel_num),\n",
    "\t\t)\n",
    "        self.relu = nn.ReLU()\n",
    "        torch.nn.init.kaiming_normal_(self.conv_block1[0].weight)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_block2[0].weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = x + residual\n",
    "        out = self.relu(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ede3560-e3f5-40d0-bc15-40d3e2160832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10_ResNetAE(nn.Module):\n",
    "    def __init__(self, in_channels = 3, type = 18):\n",
    "        super(Cifar10_ResNetAE, self).__init__()\n",
    "        self.struc_dict = {\n",
    "            18: {\n",
    "                \"num_channels\" : [48],\n",
    "                \"counts\" : [1]\n",
    "            }\n",
    "        }\n",
    "        channel_init = self.struc_dict[type][\"num_channels\"][0]\n",
    "        self.conv_in = nn.Conv2d(in_channels=in_channels, out_channels=channel_init, kernel_size=5, stride=3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_in.weight)\n",
    "        self.max_in = nn.MaxPool2d(2)\n",
    "\n",
    "        num_channels = self.struc_dict[type][\"num_channels\"]\n",
    "        counts = self.struc_dict[type][\"counts\"]\n",
    "        inv_num_channels = self.struc_dict[type][\"num_channels\"][::-1]\n",
    "        \n",
    "        self.encoder = nn.Sequential()\n",
    "        for idx, struc in enumerate(\n",
    "            zip(\n",
    "                num_channels, \n",
    "                counts\n",
    "            )\n",
    "        ):\n",
    "            num_channel, cnt = struc\n",
    "            for i in range(cnt):\n",
    "                self.encoder.add_module(f\"conv{idx}_{i}\", BasicBlock(num_channel))\n",
    "            if idx < len(num_channels) - 1:\n",
    "                self.encoder.add_module(\n",
    "                    f\"conv_switch{idx}_{idx+1}\", nn.Conv2d(num_channel, num_channels[idx+1], 3, 1)\n",
    "                )\n",
    "                self.encoder.add_module(\n",
    "                    f\"bn{idx}_{idx+1}\", nn.BatchNorm2d(num_channels[idx+1])\n",
    "                )\n",
    "        self.dw_latent = nn.Sequential(\n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(5 * 5 * 48, 256),\n",
    "        )\n",
    "        \n",
    "        self.latent = nn.Linear(256, 256)\n",
    "        \n",
    "        self.up_latent = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.Linear(256, 5 * 5 * 48),            \n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential()\n",
    "        for idx, struc in enumerate(\n",
    "            zip(\n",
    "                inv_num_channels, \n",
    "                counts\n",
    "            )\n",
    "        ):\n",
    "            num_channel, cnt = struc\n",
    "            for i in range(cnt):\n",
    "                self.decoder.add_module(f\"deconv{idx}_{i}\", BasicTransBlock(num_channel))\n",
    "            if idx < len(inv_num_channels) - 1:\n",
    "                self.decoder.add_module(\n",
    "                    f\"deconv_switch{idx}_{idx+1}\", nn.ConvTranspose2d(num_channel, inv_num_channels[idx+1], 3, 1)\n",
    "                )\n",
    "                self.decoder.add_module(\n",
    "                    f\"bn{idx}_{idx+1}\", nn.BatchNorm2d(inv_num_channels[idx+1])\n",
    "                )\n",
    "\n",
    "        self.up_out = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv_out = nn.ConvTranspose2d(in_channels=channel_init, out_channels=in_channels, kernel_size=5, stride=3)\n",
    "        torch.nn.init.kaiming_normal_(self.conv_out.weight)\n",
    "                                     \n",
    "    def forward(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        x = self.max_in(x)\n",
    "        x = self.encoder(x)\n",
    "        lat = self.latent(self.dw_latent(x))\n",
    "        mu, lv = self.unwrap(lat)\n",
    "        rec_lat = self.up_latent(self.reparam(mu, lv))\n",
    "        x = self.decoder(rec_lat.view(-1, 48, 5, 5))\n",
    "        x = self.up_out(x)\n",
    "        x = self.conv_out(x)\n",
    "        return x, mu, lv\n",
    "\n",
    "    def unwrap(self, x):\n",
    "        return torch.split(x, x.shape[1]//2, dim=1)\n",
    "\n",
    "    def reparam(self, mu, lv):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * lv)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + std * eps\n",
    "        else:\n",
    "            return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea82a86-fde3-45b0-8ab8-5cb4719941e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kls(mu, logvar, mean=False):\n",
    "\n",
    "    klds = -0.5*(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    if mean:\n",
    "        reduce = lambda x: torch.mean(x, 1)\n",
    "    else:\n",
    "        reduce = lambda x: torch.sum(x, 1)\n",
    "\n",
    "    total_kld = reduce(klds).mean(0, True)\n",
    "    dimension_wise_kld = klds.mean(0)\n",
    "    mean_kld = reduce(klds).mean(0, True)\n",
    "\n",
    "    return total_kld, dimension_wise_kld, mean_kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b28901-3fc3-40fa-a452-9d76b6e80e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\", index = 0)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "model = Cifar10_ResNetAE().to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a69eb25-f9be-43f7-8df4-8dee2f37b9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                 | 0/25 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2048x1200 and 2352x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_img, _ \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dl):\n\u001b[1;32m      5\u001b[0m     train_img \u001b[38;5;241m=\u001b[39m train_img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     gen_img, train_mu, train_lv \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     train_rec_loss \u001b[38;5;241m=\u001b[39m loss_fn(gen_img, train_img)\n\u001b[1;32m      9\u001b[0m     train_kl_loss, _, _ \u001b[38;5;241m=\u001b[39m gaussian_kls(train_mu, train_lv)\n",
      "File \u001b[0;32m~/Documents/git/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 74\u001b[0m, in \u001b[0;36mCifar10_ResNetAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_in(x)\n\u001b[1;32m     73\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[0;32m---> 74\u001b[0m lat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdw_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     75\u001b[0m mu, lv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munwrap(lat)\n\u001b[1;32m     76\u001b[0m rec_lat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_latent(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparam(mu, lv))\n",
      "File \u001b[0;32m~/Documents/git/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/git/.env/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/git/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/git/.env/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2048x1200 and 2352x256)"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    tr_total_loss = 0\n",
    "    for train_img, _ in tqdm(train_dl):\n",
    "        train_img = train_img.to(device)\n",
    "\n",
    "        gen_img, train_mu, train_lv = model(train_img)\n",
    "        train_rec_loss = loss_fn(gen_img, train_img)\n",
    "        train_kl_loss, _, _ = gaussian_kls(train_mu, train_lv)\n",
    "        train_loss = train_rec_loss + train_kl_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_total_loss += train_loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        va_total_loss = 0\n",
    "        for valid_img, _ in tqdm(test_dl):\n",
    "            valid_img = valid_img.to(device)\n",
    "\n",
    "            gen_img, valid_mu, valid_lv = model(valid_img)\n",
    "            valid_rec_loss = loss_fn(gen_img, valid_img)\n",
    "            valid_kl_loss, _, _ = gaussian_kls(valid_mu, valid_lv)\n",
    "            valid_loss = valid_rec_loss + valid_kl_loss\n",
    "\n",
    "            va_total_loss += valid_loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch} - TrainLoss: {tr_total_loss/len(train_dl)} - ValidLoss: {va_total_loss/len(test_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5deca1-058c-401d-a742-f94ab203b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    va_total_loss = 0\n",
    "    for valid_img, _ in tqdm(test_dl):\n",
    "        valid_img = valid_img.to(device)\n",
    "\n",
    "        gen_img, valid_mu, valid_lv = model(valid_img)\n",
    "        valid_rec_loss = loss_fn(gen_img, valid_img)\n",
    "        valid_kl_loss, _, _ = gaussian_kls(valid_mu, valid_lv)\n",
    "        valid_loss = valid_rec_loss + valid_kl_loss\n",
    "\n",
    "        va_total_loss += valid_loss.item()\n",
    "\n",
    "print(f\"ValidLoss: {va_total_loss/len(test_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb281eb8-1840-4834-8edb-2d526d4d7e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for test_case_idx in tqdm(range(10)):\n",
    "        valid_img, _ = testset[test_case_idx]\n",
    "        valid_img = valid_img.unsqueeze(dim=0).to(device)\n",
    "\n",
    "        gen_img, _, _ = model(valid_img)  \n",
    "        print(torch.sum(gen_img - valid_img))\n",
    "\n",
    "        f, axarr = plt.subplots(1, 2)\n",
    "        axarr[0].imshow(valid_img[0].cpu().permute(1, -1, 0).numpy())\n",
    "        axarr[1].imshow(gen_img[0].cpu().permute(1, -1, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482b18f-dbfa-4e62-962c-d900784b7bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
