{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0cdf81-f86b-498e-a1b7-c2b6a7029912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import CelebA\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b08817f6-b141-465d-97ff-398b7051263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "162770 19867 19962\n",
      "2544 311 312\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(148),\n",
    "        transforms.Resize(64),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "root_dir = '/media/mountHDD2/data'\n",
    "\n",
    "trainset = CelebA(root=root_dir, split='train', download=True, transform=transform)\n",
    "train_dl = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=24)\n",
    "validset = CelebA(root=root_dir, split='valid', download=True, transform=transform)\n",
    "valid_dl = DataLoader(validset, batch_size=64, shuffle=False, num_workers=24)\n",
    "testset = CelebA(root=root_dir, split='test', download=True, transform=transform)\n",
    "test_dl = DataLoader(testset, batch_size=64, shuffle=False, num_workers=24)\n",
    "\n",
    "print(len(trainset), len(validset), len(testset))\n",
    "print(len(train_dl), len(valid_dl), len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7803be5-33c5-4b11-b536-f4f1be5cee09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5_o_Clock_Shadow',\n",
       " 'Arched_Eyebrows',\n",
       " 'Attractive',\n",
       " 'Bags_Under_Eyes',\n",
       " 'Bald']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.attr_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550bc925-afef-449e-98a6-c419819145b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCeleb(CelebA):\n",
    "    def __init__(self, root=root_dir, split='train', download=True, transform=transform):\n",
    "        super().__init__(root=root_dir, split=split, download=download, transform=transform,\n",
    "                        target_type = ['attr', 'identity'])\n",
    "        self.attr_names = self.attr_names[:-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return super().__len__()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super().__getitem__(idx)\n",
    "\n",
    "        attr = target[0]\n",
    "        identity = target[1]\n",
    "\n",
    "        target = {\n",
    "            \"rec\" : img,\n",
    "            \"identity\" : identity\n",
    "        }\n",
    "\n",
    "        attr_dict = {\n",
    "            f\"attr_{self.attr_names[idx]}\" : attr[idx] for idx in range(attr.shape[0])\n",
    "        }\n",
    "\n",
    "        target.update(attr_dict)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62163fc-a66b-4869-a28b-795f3c894f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "162770 19867 19962\n",
      "2544 311 312\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(148),\n",
    "        transforms.Resize(64),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "root_dir = '/media/mountHDD2/data'\n",
    "\n",
    "trainset = CustomCeleb(root=root_dir, split='train', download=True, transform=transform)\n",
    "train_dl = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=24)\n",
    "validset = CustomCeleb(root=root_dir, split='valid', download=True, transform=transform)\n",
    "valid_dl = DataLoader(validset, batch_size=64, shuffle=False, num_workers=24)\n",
    "testset = CustomCeleb(root=root_dir, split='test', download=True, transform=transform)\n",
    "test_dl = DataLoader(testset, batch_size=64, shuffle=False, num_workers=24)\n",
    "\n",
    "print(len(trainset), len(validset), len(testset))\n",
    "print(len(train_dl), len(valid_dl), len(test_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a527de7-d45d-4490-aca8-09ef838586b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64, 64])\n",
      "{'rec': tensor([[[0.6353, 0.4471, 0.4118,  ..., 0.9882, 0.9843, 0.9843],\n",
      "         [0.6471, 0.5020, 0.3882,  ..., 0.9882, 0.9843, 0.9882],\n",
      "         [0.6706, 0.4980, 0.4980,  ..., 0.9882, 0.9882, 0.9882],\n",
      "         ...,\n",
      "         [0.5373, 0.2549, 0.2510,  ..., 0.5765, 0.5490, 0.5333],\n",
      "         [0.5490, 0.2275, 0.2471,  ..., 0.6275, 0.5765, 0.5294],\n",
      "         [0.5137, 0.2627, 0.2902,  ..., 0.6078, 0.5804, 0.5137]],\n",
      "\n",
      "        [[0.4824, 0.2549, 0.2196,  ..., 0.9373, 0.9333, 0.9255],\n",
      "         [0.5059, 0.3098, 0.1922,  ..., 0.9373, 0.9333, 0.9373],\n",
      "         [0.5216, 0.3137, 0.3098,  ..., 0.9373, 0.9373, 0.9333],\n",
      "         ...,\n",
      "         [0.3608, 0.1216, 0.0863,  ..., 0.3490, 0.3137, 0.3059],\n",
      "         [0.3686, 0.0980, 0.0706,  ..., 0.3843, 0.3373, 0.3020],\n",
      "         [0.3373, 0.1216, 0.0980,  ..., 0.3569, 0.3412, 0.2863]],\n",
      "\n",
      "        [[0.3255, 0.1373, 0.1176,  ..., 0.8039, 0.8000, 0.7961],\n",
      "         [0.3137, 0.1882, 0.1020,  ..., 0.8078, 0.8039, 0.8078],\n",
      "         [0.3373, 0.1922, 0.2196,  ..., 0.8118, 0.8118, 0.8078],\n",
      "         ...,\n",
      "         [0.2941, 0.0980, 0.0549,  ..., 0.2471, 0.2118, 0.1961],\n",
      "         [0.3020, 0.0706, 0.0353,  ..., 0.2588, 0.2275, 0.1922],\n",
      "         [0.2588, 0.0863, 0.0549,  ..., 0.2039, 0.2196, 0.1686]]]), 'identity': tensor(2880), 'attr_5_o_Clock_Shadow': tensor(0), 'attr_Arched_Eyebrows': tensor(1), 'attr_Attractive': tensor(1), 'attr_Bags_Under_Eyes': tensor(0), 'attr_Bald': tensor(0), 'attr_Bangs': tensor(0), 'attr_Big_Lips': tensor(0), 'attr_Big_Nose': tensor(0), 'attr_Black_Hair': tensor(0), 'attr_Blond_Hair': tensor(0), 'attr_Blurry': tensor(0), 'attr_Brown_Hair': tensor(1), 'attr_Bushy_Eyebrows': tensor(0), 'attr_Chubby': tensor(0), 'attr_Double_Chin': tensor(0), 'attr_Eyeglasses': tensor(0), 'attr_Goatee': tensor(0), 'attr_Gray_Hair': tensor(0), 'attr_Heavy_Makeup': tensor(1), 'attr_High_Cheekbones': tensor(1), 'attr_Male': tensor(0), 'attr_Mouth_Slightly_Open': tensor(1), 'attr_Mustache': tensor(0), 'attr_Narrow_Eyes': tensor(0), 'attr_No_Beard': tensor(1), 'attr_Oval_Face': tensor(0), 'attr_Pale_Skin': tensor(0), 'attr_Pointy_Nose': tensor(1), 'attr_Receding_Hairline': tensor(0), 'attr_Rosy_Cheeks': tensor(0), 'attr_Sideburns': tensor(0), 'attr_Smiling': tensor(1), 'attr_Straight_Hair': tensor(1), 'attr_Wavy_Hair': tensor(0), 'attr_Wearing_Earrings': tensor(1), 'attr_Wearing_Hat': tensor(0), 'attr_Wearing_Lipstick': tensor(1), 'attr_Wearing_Necklace': tensor(0), 'attr_Wearing_Necktie': tensor(0), 'attr_Young': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "sample_img, sample_target = trainset[0]\n",
    "\n",
    "print(sample_img.shape)\n",
    "print(sample_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b16f7ca-040c-440c-a650-ad9948506be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 64, 64])\n",
      "torch.Size([64, 3, 64, 64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for img, target in train_dl:\n",
    "    print(img.shape)\n",
    "    print(target[\"rec\"].shape)\n",
    "    for key in target:\n",
    "        if key == \"rec\":\n",
    "            continue\n",
    "        else:\n",
    "            print(target[key].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adcdd487-4546-4990-94ee-3596877e149f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2544/2544 [07:13<00:00,  5.87it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 311/311 [00:04<00:00, 67.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 312/312 [00:04<00:00, 69.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for img, target in tqdm(train_dl):\n",
    "    pass\n",
    "for img, target in tqdm(valid_dl):\n",
    "    pass\n",
    "for img, target in tqdm(test_dl):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968521c9-cb1a-4fc4-a19c-7753967ae150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
