{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "349134bd-3d0f-48bd-8576-f75a7e12ee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import Cityscapes\n",
    "from torchvision import transforms\n",
    "import torchvision as tv\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a306e363-ccaa-4991-a8c8-6163c528ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0 : ['unlabeled', 255, 'void'], \n",
    "    1 : ['ego vehicle', 255, 'void'],\n",
    "    2 : ['rectification border', 255, 'void'],\n",
    "    3 : ['out of roi', 255, 'void'],\n",
    "    4 : ['static', 255, 'void'],\n",
    "    5 : ['dynamic', 255, 'void'],\n",
    "    6 : ['ground', 255, 'void'],\n",
    "    7 : ['road', 0, 'flat'],\n",
    "    8 : ['sidewalk', 1, 'flat'],\n",
    "    9 : ['parking', 255, 'flat'],\n",
    "    10 : ['rail track', 255, 'flat'],\n",
    "    11 : ['building', 2, 'construction'],\n",
    "    12 : ['wall', 3, 'construction'],\n",
    "    13 : ['fence', 4, 'construction'],\n",
    "    14 : ['guard rail', 255, 'construction'],\n",
    "    15 : ['bridge', 255, 'construction'],\n",
    "    16 : ['tunnel', 255, 'construction'],\n",
    "    17 : ['pole', 5, 'object'],\n",
    "    18 : ['polegroup', 255, 'object'],\n",
    "    19 : ['traffic light', 6, 'object'],\n",
    "    20 : ['traffic sign', 7, 'object'],\n",
    "    21 : ['vegetation', 8, 'nature'],\n",
    "    22 : ['terrain', 9, 'nature'],\n",
    "    23 : ['sky', 10, 'sky'],\n",
    "    24 : ['person', 11, 'human'],\n",
    "    25 : ['rider', 12, 'human'],\n",
    "    26 : ['car', 13, 'vehicle'],\n",
    "    27 : ['truck', 14, 'vehicle'],\n",
    "    28 : ['bus', 15, 'vehicle'],\n",
    "    29 : ['caravan', 255, 'vehicle'],\n",
    "    30 : ['trailer', 255, 'vehicle'],\n",
    "    31 : ['train', 16, 'vehicle'],\n",
    "    32 : ['motorcycle', 17, 'vehicle'],\n",
    "    33 : ['bicycle', 18, 'vehicle'],\n",
    "    34 : ['license plate', -1, 'vehicle']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47633009-94de-4dd9-b614-9b542ff799b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encx = torch.zeros(x.shape, dtype=torch.long)\n",
    "        for label in label_map:\n",
    "            encx[x == label] = label_map[label][1] if label_map[label][1] != 255 else 19\n",
    "        return F.one_hot(encx.squeeze(1), 20).permute(0, 3, 1, 2)[0].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527e3bdf-0740-44ea-b964-c51e88b90e5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or incomplete. Please make sure all required folders for the specified \"split\" and \"mode\" are inside the \"root\" directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m      8\u001b[0m target_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m      9\u001b[0m     [\n\u001b[1;32m     10\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     ]\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m Cityscapes(\n\u001b[1;32m     17\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/mountHDD2/cityscapes\u001b[39m\u001b[38;5;124m\"\u001b[39m, split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoarse\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     18\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transform, \n\u001b[1;32m     19\u001b[0m     target_transform \u001b[38;5;241m=\u001b[39m target_transform\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m valid_ds \u001b[38;5;241m=\u001b[39m \u001b[43mCityscapes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/media/mountHDD2/cityscapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_extra\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoarse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msemantic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m Cityscapes(\n\u001b[1;32m     29\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/mountHDD2/cityscapes\u001b[39m\u001b[38;5;124m\"\u001b[39m, split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoarse\u001b[39m\u001b[38;5;124m\"\u001b[39m, target_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     30\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transform, \n\u001b[1;32m     31\u001b[0m     target_transform \u001b[38;5;241m=\u001b[39m target_transform\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m train_dl \u001b[38;5;241m=\u001b[39m DataLoader(train_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/git/.env/lib/python3.10/site-packages/torchvision/datasets/cityscapes.py:155\u001b[0m, in \u001b[0;36mCityscapes.__init__\u001b[0;34m(self, root, split, mode, target_type, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m    153\u001b[0m         extract_archive(from_path\u001b[38;5;241m=\u001b[39mtarget_dir_zip, to_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or incomplete. Please make sure all required folders for the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m specified \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are inside the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m directory\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir):\n\u001b[1;32m    161\u001b[0m     img_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir, city)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found or incomplete. Please make sure all required folders for the specified \"split\" and \"mode\" are inside the \"root\" directory"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 512)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 512)),\n",
    "        transforms.PILToTensor(), \n",
    "        MakeClass()\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = Cityscapes(\n",
    "    root = \"/media/mountHDD2/cityscapes\", split = \"train\", mode = \"coarse\", target_type = \"semantic\", \n",
    "    transform = transform, \n",
    "    target_transform = target_transform\n",
    ")\n",
    "\n",
    "valid_ds = Cityscapes(\n",
    "    root = \"/media/mountHDD2/cityscapes\", split = \"train_extra\", mode = \"coarse\", target_type = \"semantic\", \n",
    "    transform = transform, \n",
    "    target_transform = target_transform    \n",
    ")\n",
    "\n",
    "test_ds = Cityscapes(\n",
    "    root = \"/media/mountHDD2/cityscapes\", split = \"val\", mode = \"coarse\", target_type = \"semantic\", \n",
    "    transform = transform, \n",
    "    target_transform = target_transform\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, pin_memory = True, num_workers=24)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=True, pin_memory = True, num_workers=24)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=True, pin_memory = True, num_workers=24)\n",
    "\n",
    "print(\"#Training Samples: {}\".format(len(train_ds)))\n",
    "print(\"#Validation Samples: {}\".format(len(valid_ds)))\n",
    "print(\"#Testing Samples: {}\".format(len(test_ds)))\n",
    "print(\"#Training Batch: {}\".format(len(train_dl)))\n",
    "print(\"#Validation Batch: {}\".format(len(valid_dl)))\n",
    "print(\"#Testing Batch: {}\".format(len(test_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6ca64-e463-4575-ba1f-7938f37a737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data, sample_mask = train_ds[19]\n",
    "total_num_class = 20\n",
    "\n",
    "print(f\"Total #Classes: {total_num_class}\")\n",
    "print(f\"data shape: {sample_data.shape}\")\n",
    "print(f\"mask shape: {sample_mask.shape}\")\n",
    "num_class = sample_mask.shape[0]\n",
    "print(f\"Number classes: {num_class}\")\n",
    "print(f\"Classes: {torch.unique(torch.argmax(sample_mask, dim = 0))}\")\n",
    "\n",
    "f, axarr = plt.subplots(1, 2, figsize=(20, 15))\n",
    "axarr[0].imshow(sample_data.permute(1, -1, 0).numpy())\n",
    "axarr[1].imshow(torch.argmax(sample_mask, dim = 0).unsqueeze(0).permute(1, -1, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f049575-6e34-4384-808e-3c181322ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00368a00-c45e-49f0-985a-0a2fc42915ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6de6d-b3e1-4c46-87a5-8d64d6b79caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):    \n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff597763-347b-40d7-963e-8c539f266077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4390392b-5676-4f1e-b9dd-8bce3c0c06e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 32))\n",
    "        self.down1 = (Down(32, 64)) #64, 128\n",
    "        self.down2 = (Down(64, 128)) #128, 256\n",
    "        self.down3 = (Down(128, 256)) #256, 512\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(256, 512 // factor)) #512, 1024\n",
    "        self.up1 = (Up(512, 256 // factor, bilinear)) #1024, 512\n",
    "        self.up2 = (Up(256, 128 // factor, bilinear)) #512, 256\n",
    "        self.up3 = (Up(128, 64 // factor, bilinear)) #256, 128\n",
    "        self.up4 = (Up(64, 32, bilinear)) #128, 64\n",
    "        self.outc = (OutConv(32, n_classes)) #64\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3fc64-fc1a-4899-816a-2e204e78eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all batches, or for a single mask\n",
    "    assert input.size() == target.size()\n",
    "    assert input.dim() == 3 or not reduce_batch_first\n",
    "\n",
    "    sum_dim = (-1, -2) if input.dim() == 2 or not reduce_batch_first else (-1, -2, -3)\n",
    "\n",
    "    inter = 2 * (input * target).sum(dim=sum_dim)\n",
    "    sets_sum = input.sum(dim=sum_dim) + target.sum(dim=sum_dim)\n",
    "    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
    "\n",
    "    dice = (inter + epsilon) / (sets_sum + epsilon)\n",
    "    return dice.mean()\n",
    "\n",
    "\n",
    "def multiclass_dice_coeff(input: Tensor, target: Tensor, reduce_batch_first: bool = False, epsilon: float = 1e-6):\n",
    "    # Average of Dice coefficient for all classes\n",
    "    return dice_coeff(input.flatten(0, 1), target.flatten(0, 1), reduce_batch_first, epsilon)\n",
    "\n",
    "\n",
    "def dice_loss(input: Tensor, target: Tensor, multiclass: bool = False):\n",
    "    # Dice loss (objective to minimize) between 0 and 1\n",
    "    fn = multiclass_dice_coeff if multiclass else dice_coeff\n",
    "    return 1 - fn(input, target, reduce_batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3cbda8-8853-46b5-a2d2-b5cb898bc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\", index = 0)\n",
    "\n",
    "model = UNet(3, total_num_class, True).to(device)\n",
    "\n",
    "optimizer = Adam(params = model.parameters(), lr = 0.001)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer, epochs * len(train_dl))\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900491a-589a-4921-a14a-83394e21f13f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "old_loss = 1e26\n",
    "best_dct = None\n",
    "last_dst = None\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    tr_total_loss = 0\n",
    "    for train_img, train_mask in tqdm(train_dl):\n",
    "        train_img = train_img.to(device)\n",
    "        train_mask = train_mask.to(device)\n",
    "\n",
    "        train_gen_mask = model(train_img)\n",
    "        train_rec_loss = loss_fn(train_gen_mask, train_mask)\n",
    "        train_dice_loss = dice_loss(\n",
    "            F.softmax(train_gen_mask, dim=1).float(),\n",
    "            train_mask.float(),\n",
    "            multiclass=True\n",
    "        )\n",
    "        train_loss = train_rec_loss + train_dice_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        tr_total_loss += train_loss.cpu().item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        va_total_loss = 0\n",
    "        for valid_img, valid_mask in tqdm(valid_dl):\n",
    "            valid_img = valid_img.to(device)\n",
    "            valid_mask = valid_mask.to(device)\n",
    "            \n",
    "            valid_gen_mask = model(valid_img)\n",
    "            valid_loss = loss_fn(valid_gen_mask, valid_mask)\n",
    "            valid_dice_loss = dice_loss(\n",
    "                F.softmax(valid_gen_mask, dim=1).float(),\n",
    "                valid_mask.float(),\n",
    "                multiclass=True\n",
    "            )\n",
    "            valid_loss = train_rec_loss + train_dice_loss\n",
    "\n",
    "            va_total_loss += valid_loss.cpu().item()\n",
    "            \n",
    "    mean_train_loss = tr_total_loss/len(train_dl)\n",
    "    mean_valid_loss = va_total_loss/len(test_dl)\n",
    "\n",
    "    if mean_valid_loss <= old_loss:\n",
    "        old_loss = mean_valid_loss\n",
    "        best_dct = model.state_dict()\n",
    "    \n",
    "    last_dct = model.state_dict()\n",
    "\n",
    "    print(f\"Epoch: {epoch} - TrainLoss: {mean_train_loss} - ValidLoss: {mean_valid_loss}\")\n",
    "model.load_state_dict(best_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba745c1e-4259-40a9-87fb-9ffb02f1d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_case_idx in range(10):\n",
    "        valid_img, valid_mask = test_ds[test_case_idx]\n",
    "        valid_img = valid_img.unsqueeze(dim=0).to(device)\n",
    "\n",
    "        gen_mask = model(valid_img)     \n",
    "        # f, axarr = plt.subplots(1, total_num_class + 3, figsize = (20,15))\n",
    "        f, axarr = plt.subplots(1, 2, figsize = (20,15))\n",
    "        axarr[0].imshow(valid_img[0].cpu().permute(1, -1, 0).numpy())\n",
    "        axarr[0].axis('off')\n",
    "        axarr[1].imshow(torch.argmax(gen_mask[0], dim=0).cpu().unsqueeze(0).permute(1, -1, 0).numpy())\n",
    "        axarr[1].axis('off')\n",
    "        # for idx in range(total_num_class):\n",
    "        #     axarr[idx + 2].imshow(gen_mask[0][idx].unsqueeze(0).cpu().permute(1, -1, 0).numpy())\n",
    "        #     axarr[idx + 2].axis('off')\n",
    "        # axarr[total_num_class+2].imshow(torch.argmax(valid_mask, dim=0).unsqueeze(0).cpu().permute(1, -1, 0).numpy())\n",
    "        # axarr[total_num_class+2].axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
